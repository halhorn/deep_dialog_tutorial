{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNLM\n",
    "Recurrent Neural Network Language Model\n",
    "RNN ã«ã‚ˆã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
    "æ–‡ç« ã®é›†å›£ã‚’å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã§ã€ãã‚Œã£ã½ã„æ–‡ç« ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚\n",
    "\n",
    "ã“ã‚ŒãŒç™ºå±•ã—ã¦ Seq2Seq ã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼éƒ¨åˆ†ã«ãªã£ã¦ã„ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 1024\n",
    "embedding_dim = 256\n",
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¥å‡ºåŠ›éƒ¨åˆ†\n",
    "in_ph = tf.placeholder(tf.int32, shape=[None, None], name='in_ph')\n",
    "out_ph = tf.placeholder(tf.int32, shape=[None, None], name='out_ph')\n",
    "len_ph = tf.placeholder(tf.int32, shape=[None], name='len_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(ops):\n",
    "    '''ä¸ãˆã‚‰ã‚ŒãŸè¨ˆç®—ãƒãƒ¼ãƒ‰ã®å€¤ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚'''\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        result = sess.run(ops, {\n",
    "            in_ph: [[30, 40, 50], [160, 170, 180]],\n",
    "            out_ph:[[40, 50, 60], [170, 180, 190]],\n",
    "            len_ph:[3, 3]\n",
    "        })\n",
    "        print('## {}\\nshape: {}'.format(ops.name, ops.shape))\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## embedding_lookup:0\n",
      "shape: (?, ?, 256)\n",
      "[[[ 1.29957652 -1.96133733  1.12529576 ..., -0.31777608  1.02436149\n",
      "   -0.51166236]\n",
      "  [-0.61737949  0.64050382  0.6832155  ...,  1.7489109   0.01848829\n",
      "    0.97201866]\n",
      "  [-0.34090382  0.01002835  1.27707613 ..., -0.09280876 -0.12294848\n",
      "    0.98473871]]\n",
      "\n",
      " [[ 0.73209351 -0.03378905 -0.79808235 ..., -0.26269943 -0.58393425\n",
      "    0.44871616]\n",
      "  [-0.44863656 -0.54927611 -0.68908316 ...,  0.24833307  0.09024531\n",
      "   -0.85815227]\n",
      "  [-1.61067867  0.65220904 -1.02846551 ..., -1.07364619  0.79616362\n",
      "   -0.40707001]]]\n"
     ]
    }
   ],
   "source": [
    "# embeddings - æ–‡å­—ã® ID ã‹ã‚‰åˆ†æ•£è¡¨ç¾ã®ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚\n",
    "# ãƒ‡ãƒ¼ã‚¿ã¯ [batch_size, sentence_len, embedding_dim] ã®å½¢ã«ãªã‚Šã¾ã™ã€‚\n",
    "embeddings = tf.Variable(tf.random_normal([vocab_size, embedding_dim], stddev=1), name='embeddings', dtype=tf.float32)\n",
    "in_embedded = tf.nn.embedding_lookup(embeddings, in_ph)\n",
    "debug(in_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## output_layer/Tensordot:0\n",
      "shape: (?, ?, 1000)\n",
      "[[[ -4.15025838e-02  -7.28277862e-02  -5.71157821e-02 ...,  -4.88833077e-02\n",
      "     6.67364672e-02  -1.00397669e-01]\n",
      "  [  9.57280584e-03  -2.16150463e-01   1.45810060e-02 ...,  -2.24354371e-01\n",
      "     8.15779939e-02  -7.00069293e-02]\n",
      "  [  7.99868181e-02  -1.87654123e-01   2.42325097e-01 ...,  -1.22888096e-01\n",
      "    -3.97441238e-02   1.01946015e-02]]\n",
      "\n",
      " [[ -3.20171192e-02   6.15284815e-02   1.94854960e-01 ...,   2.40536220e-02\n",
      "     1.37256868e-02   6.42112829e-03]\n",
      "  [ -1.77193165e-01   1.88514516e-02   2.11725533e-01 ...,  -2.90628672e-02\n",
      "    -6.70165196e-02  -9.68019143e-02]\n",
      "  [ -6.32315874e-02   4.80010733e-02   2.87601769e-01 ...,   4.76051271e-02\n",
      "    -2.30992213e-04  -1.15126424e-01]]]\n"
     ]
    }
   ],
   "source": [
    "# RNN éƒ¨åˆ†\n",
    "cell = tf.nn.rnn_cell.GRUCell(hidden_dim, kernel_initializer=tf.orthogonal_initializer)\n",
    "rnn_out, final_state = tf.nn.dynamic_rnn(\n",
    "    cell=cell,\n",
    "    inputs=in_embedded,\n",
    "    sequence_length=len_ph,\n",
    "    dtype=tf.float32,\n",
    "    scope='rnn',\n",
    ")\n",
    "# éš ã‚Œå±¤ã‹ã‚‰å…¨çµåˆã‚’ã‹ã¾ã›ã¦ã€å„å˜èªã®ç”Ÿæˆç¢ºç‡ã£ã½ã„å€¤ã«ã™ã‚‹ã€‚\n",
    "# ï¼ˆiç•ªç›®ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å‡ºåŠ›ãŒ id: i ã®å˜èªã®ç”Ÿæˆç¢ºç‡ã£ã½ã„ã‚‚ã®ã«ãªã‚‹ï¼‰\n",
    "output_layer = layers_core.Dense(vocab_size, use_bias=False, name='output_layer')\n",
    "onehot_logits = output_layer.apply(rnn_out)\n",
    "debug(onehot_logits)\n",
    "output_ids_op = tf.argmax(onehot_logits, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ™‚ç”¨ã® RNN\n",
    "beam_width = 20\n",
    "gen_max_len = 500\n",
    "start_token = random.randrange(0, vocab_size)\n",
    "\n",
    "decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "    cell=cell,\n",
    "    embedding=embeddings,\n",
    "    start_tokens=tf.ones([1], tf.int32) * start_token,  # ç”Ÿæˆæ™‚ã® batch_size ã¯1\n",
    "    end_token=0,  # dummy\n",
    "    initial_state=cell.zero_state(beam_width, tf.float32),\n",
    "    beam_width=beam_width,\n",
    "    output_layer=output_layer,\n",
    ")\n",
    "\n",
    "beam_decoder_output = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder=decoder,\n",
    "    maximum_iterations=500,\n",
    "    scope='generator_decode'\n",
    ")[0]\n",
    "generate_op = beam_decoder_output.predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loss:0\n",
      "shape: ()\n",
      "6.94956\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=out_ph,\n",
    "    logits=onehot_logits,\n",
    ")\n",
    "loss_op = tf.reduce_mean(cross_entropy, name='loss')\n",
    "debug(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Convert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = 50\n",
    "batch_size = 512\n",
    "data_path = 'data/natsume.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.rev_dict = {c: i for i, c in enumerate(vocab)}\n",
    "        self.pad = 0\n",
    "        self.bos = 1\n",
    "        self.eos = 2\n",
    "        self.unk = 3\n",
    "    \n",
    "    @classmethod\n",
    "    def from_text(cls, text):\n",
    "        char_freq_tuples = collections.Counter(text).most_common(vocab_size - 4)\n",
    "        vocab, _ = zip(*char_freq_tuples)\n",
    "        vocab = ['<pad>', '<bos>', '<eos>', '<unk>'] + list(vocab)\n",
    "        return cls(vocab)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab_size)\n",
    "    \n",
    "    def text2id(self, text):\n",
    "        return [self.rev_dict[c] if c in self.rev_dict else self.unk for c in text]\n",
    "\n",
    "    def id2text(self, ids):\n",
    "        return ''.join(self.vocab[i] for i in ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as f:\n",
    "    text = f.read().replace('\\n', '')\n",
    "\n",
    "tokenizer = Tokenizer.from_text(text)\n",
    "ids = tokenizer.text2id(text)\n",
    "\n",
    "def split_ndlist(ndlist, size):\n",
    "    return [np.array(ndlist[i - size:i]) for i in range(size, len(ndlist) + 1, size)]\n",
    "\n",
    "# (1æ–‡å­—ç›®, 2æ–‡å­—ç›®), (2æ–‡å­—ç›®, 3æ–‡å­—ç›®), ... ã¨ã„ã†ãƒšã‚¢ã‚’ä½œã‚‹\n",
    "# ã‚ã‚‹æ™‚åˆ»ã®å…¥åŠ›ã«å¯¾ã—ãã®æ¬¡æ™‚åˆ»ã®å‡ºåŠ›ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚\n",
    "in_sequence_list = split_ndlist(ids[:-1], size=sentence_len)\n",
    "out_sequence_list = split_ndlist(ids[1:], size=sentence_len)\n",
    "\n",
    "in_batch_list = split_ndlist(in_sequence_list, batch_size)\n",
    "out_batch_list = split_ndlist(out_sequence_list, batch_size)\n",
    "\n",
    "# batch_size å€‹ã”ã¨ã«åˆ‡ã‚Šåˆ†ã‘\n",
    "batch_list = [\n",
    "    {\n",
    "        'in': in_batch,\n",
    "        'out': out_batch,\n",
    "        'len': np.array([len(seq) for seq in in_batch]),\n",
    "    }\n",
    "    for in_batch, out_batch\n",
    "    in zip(in_batch_list, out_batch_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 27, 8, 51, 14, 3]\n",
      "ã“ã‚“ã«ã¡ã¯<unk>\n",
      "batch list num: 129\n",
      "{'out': array([[ 77,   8,   3, ..., 224,  38,  12],\n",
      "       [ 16,  55,  46, ..., 317,  14, 491],\n",
      "       [  3, 120,   3, ...,  25,  18,   7],\n",
      "       ..., \n",
      "       [ 25,  12,  10, ..., 255, 165,  11],\n",
      "       [ 23,   4,  19, ...,  49, 266,  30],\n",
      "       [ 12,  15,  13, ...,  14,  55, 109]]), 'len': array([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50]), 'in': array([[  3,  77,   8, ...,  17, 224,  38],\n",
      "       [ 12,  16,  55, ...,   4, 317,  14],\n",
      "       [491,   3, 120, ...,  27,  25,  18],\n",
      "       ..., \n",
      "       [ 19,  25,  12, ..., 190, 255, 165],\n",
      "       [ 11,  23,   4, ...,  10,  49, 266],\n",
      "       [ 30,  12,  15, ...,   4,  14,  55]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.text2id('ã“ã‚“ã«ã¡ã¯ğŸ˜'))\n",
    "print(tokenizer.id2text([33, 27, 8, 51, 14, 3]))\n",
    "print('batch list num: {}'.format(len(batch_list)))\n",
    "print(batch_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 50\n",
    "save_path = 'tmp/rnnlm/model.ckpt'\n",
    "log_dir = 'tmp/rnnlm/log/'\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.dirname(save_path)):\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss_op, global_step=global_step)\n",
    "tf.summary.scalar('loss', loss_op)\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 100000.0\n",
    "sess = tf.Session()\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(max_epoch):\n",
    "    random.shuffle(batch_list)\n",
    "    for batch in batch_list:\n",
    "        feed_dict = {\n",
    "            in_ph: batch['in'],\n",
    "            out_ph: batch['out'],\n",
    "            len_ph: batch['len'],\n",
    "        }\n",
    "        _, loss, summary, step = sess.run([train_op, loss_op, summary_op, global_step], feed_dict)\n",
    "        summary_writer.add_summary(summary, step)\n",
    "        if loss < min_loss:\n",
    "            saver.save(sess, save_path)\n",
    "            min_loss = loss\n",
    "    print('epoch {}/{} - loss: {}'.format(epoch, max_epoch, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = sess.run(generate_op)[0, :, 0]\n",
    "generated_text = tokenizer.id2text(generated_ids)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
