{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNLM\n",
    "Recurrent Neural Network Language Model\n",
    "RNN ã«ã‚ˆã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
    "æ–‡ç« ã®é›†å›£ã‚’å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã§ã€ãã‚Œã£ã½ã„æ–‡ç« ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚\n",
    "\n",
    "ã“ã‚ŒãŒç™ºå±•ã—ã¦ Seq2Seq ã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼éƒ¨åˆ†ã«ãªã£ã¦ã„ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 1024\n",
    "embedding_dim = 256\n",
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¥å‡ºåŠ›éƒ¨åˆ†\n",
    "in_ph = tf.placeholder(tf.int32, shape=[None, None], name='in_ph')\n",
    "out_ph = tf.placeholder(tf.int32, shape=[None, None], name='out_ph')\n",
    "len_ph = tf.placeholder(tf.int32, shape=[None], name='len_ph')\n",
    "gen_start_token_ph = tf.placeholder(tf.int32, shape=[], name='gen_start_token_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(ops):\n",
    "    '''ä¸ãˆã‚‰ã‚ŒãŸè¨ˆç®—ãƒãƒ¼ãƒ‰ã®å€¤ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚'''\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        result = sess.run(ops, {\n",
    "            in_ph: [[30, 40, 50], [160, 170, 180]],\n",
    "            out_ph:[[40, 50, 60], [170, 180, 190]],\n",
    "            len_ph:[3, 3]\n",
    "        })\n",
    "        print('## {}\\nshape: {}'.format(ops.name, ops.shape))\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## embedding_lookup:0\n",
      "shape: (?, ?, 256)\n",
      "[[[-1.51875842 -0.9256826   1.67430234 ..., -0.39987257  0.12857436\n",
      "   -0.55193132]\n",
      "  [ 1.42736435 -1.72631562 -0.07338642 ...,  1.14886475 -1.46833229\n",
      "   -0.39282385]\n",
      "  [ 0.2599524  -0.49925283 -0.35491866 ...,  0.16615726  1.20080113\n",
      "    2.04619074]]\n",
      "\n",
      " [[ 0.07657067  0.82344168 -0.09656296 ..., -0.41994748 -0.46351427\n",
      "    0.40188897]\n",
      "  [-0.65720195  0.70287919  0.63894361 ..., -1.8827759   1.86026025\n",
      "    1.41338992]\n",
      "  [ 0.17521305  0.81392652  0.81880534 ..., -0.94512004  0.80815339\n",
      "   -0.29899299]]]\n"
     ]
    }
   ],
   "source": [
    "# embeddings - æ–‡å­—ã® ID ã‹ã‚‰åˆ†æ•£è¡¨ç¾ã®ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚\n",
    "# ãƒ‡ãƒ¼ã‚¿ã¯ [batch_size, sentence_len, embedding_dim] ã®å½¢ã«ãªã‚Šã¾ã™ã€‚\n",
    "embeddings = tf.Variable(tf.random_normal([vocab_size, embedding_dim], stddev=1), name='embeddings', dtype=tf.float32)\n",
    "in_embedded = tf.nn.embedding_lookup(embeddings, in_ph)\n",
    "debug(in_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## output_layer/Tensordot:0\n",
      "shape: (?, ?, 1000)\n",
      "[[[ 0.00221155  0.09382153  0.03604724 ..., -0.22709875 -0.06939974\n",
      "   -0.18812199]\n",
      "  [-0.06599759  0.1990689   0.02569382 ..., -0.1297736  -0.07503378\n",
      "   -0.27143532]\n",
      "  [-0.00870639  0.11945572  0.25159195 ..., -0.03676797 -0.10855646\n",
      "    0.0134067 ]]\n",
      "\n",
      " [[ 0.01965541  0.00715457  0.05620908 ..., -0.12843172  0.17167172\n",
      "    0.14248312]\n",
      "  [ 0.02611106 -0.30134672  0.17909557 ...,  0.03745769  0.08064837\n",
      "    0.19177547]\n",
      "  [ 0.1086482  -0.21623132  0.19076024 ..., -0.07495454  0.11627872\n",
      "    0.3164745 ]]]\n"
     ]
    }
   ],
   "source": [
    "# RNN éƒ¨åˆ†\n",
    "cell = tf.nn.rnn_cell.GRUCell(hidden_dim, kernel_initializer=tf.orthogonal_initializer)\n",
    "rnn_out, final_state = tf.nn.dynamic_rnn(\n",
    "    cell=cell,\n",
    "    inputs=in_embedded,\n",
    "    sequence_length=len_ph,\n",
    "    dtype=tf.float32,\n",
    "    scope='rnn',\n",
    ")\n",
    "# éš ã‚Œå±¤ã‹ã‚‰å…¨çµåˆã‚’ã‹ã¾ã›ã¦ã€å„å˜èªã®ç”Ÿæˆç¢ºç‡ã£ã½ã„å€¤ã«ã™ã‚‹ã€‚\n",
    "# ï¼ˆiç•ªç›®ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å‡ºåŠ›ãŒ id: i ã®å˜èªã®ç”Ÿæˆç¢ºç‡ã£ã½ã„ã‚‚ã®ã«ãªã‚‹ï¼‰\n",
    "output_layer = layers_core.Dense(vocab_size, use_bias=False, name='output_layer')\n",
    "onehot_logits = output_layer.apply(rnn_out)\n",
    "debug(onehot_logits)\n",
    "output_ids_op = tf.argmax(onehot_logits, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loss:0\n",
      "shape: ()\n",
      "6.93211\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=out_ph,\n",
    "    logits=onehot_logits,\n",
    ")\n",
    "loss_op = tf.reduce_mean(cross_entropy, name='loss')\n",
    "debug(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ™‚ç”¨ã® RNN\n",
    "beam_width = 20\n",
    "gen_max_len = 500\n",
    "start_tokens = tf.ones([1], tf.int32) * gen_start_token_ph  # ç”Ÿæˆæ™‚ã® batch_size ã¯1\n",
    "\n",
    "decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "    cell=cell,\n",
    "    embedding=embeddings,\n",
    "    start_tokens=start_tokens,  \n",
    "    end_token=0,  # dummy\n",
    "    initial_state=cell.zero_state(beam_width, tf.float32),\n",
    "    beam_width=beam_width,\n",
    "    output_layer=output_layer,\n",
    ")\n",
    "\n",
    "beam_decoder_output = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder=decoder,\n",
    "    maximum_iterations=500,\n",
    "    scope='generator_decode'\n",
    ")[0]\n",
    "generate_op = beam_decoder_output.predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Convert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = 50\n",
    "batch_size = 512\n",
    "data_path = 'data/natsume.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.rev_dict = {c: i for i, c in enumerate(vocab)}\n",
    "        self.pad = 0\n",
    "        self.bos = 1\n",
    "        self.eos = 2\n",
    "        self.unk = 3\n",
    "    \n",
    "    @classmethod\n",
    "    def from_text(cls, text):\n",
    "        char_freq_tuples = collections.Counter(text).most_common(vocab_size - 4)\n",
    "        vocab, _ = zip(*char_freq_tuples)\n",
    "        vocab = ['<pad>', '<bos>', '<eos>', '<unk>'] + list(vocab)\n",
    "        return cls(vocab)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab_size)\n",
    "    \n",
    "    def text2id(self, text):\n",
    "        return [self.rev_dict[c] if c in self.rev_dict else self.unk for c in text]\n",
    "\n",
    "    def id2text(self, ids):\n",
    "        return ''.join(self.vocab[i] for i in ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as f:\n",
    "    text = f.read().replace('\\n', '')\n",
    "\n",
    "tokenizer = Tokenizer.from_text(text)\n",
    "ids = tokenizer.text2id(text)\n",
    "\n",
    "def split_ndlist(ndlist, size):\n",
    "    return [np.array(ndlist[i - size:i]) for i in range(size, len(ndlist) + 1, size)]\n",
    "\n",
    "# (1æ–‡å­—ç›®, 2æ–‡å­—ç›®), (2æ–‡å­—ç›®, 3æ–‡å­—ç›®), ... ã¨ã„ã†ãƒšã‚¢ã‚’ä½œã‚‹\n",
    "# ã‚ã‚‹æ™‚åˆ»ã®å…¥åŠ›ã«å¯¾ã—ãã®æ¬¡æ™‚åˆ»ã®å‡ºåŠ›ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚\n",
    "in_sequence_list = split_ndlist(ids[:-1], size=sentence_len)\n",
    "out_sequence_list = split_ndlist(ids[1:], size=sentence_len)\n",
    "\n",
    "in_batch_list = split_ndlist(in_sequence_list, batch_size)\n",
    "out_batch_list = split_ndlist(out_sequence_list, batch_size)\n",
    "\n",
    "# batch_size å€‹ã”ã¨ã«åˆ‡ã‚Šåˆ†ã‘\n",
    "batch_list = [\n",
    "    {\n",
    "        'in': in_batch,\n",
    "        'out': out_batch,\n",
    "        'len': np.array([len(seq) for seq in in_batch]),\n",
    "    }\n",
    "    for in_batch, out_batch\n",
    "    in zip(in_batch_list, out_batch_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 27, 8, 51, 14, 3]\n",
      "ã“ã‚“ã«ã¡ã¯<unk>\n",
      "batch list num: 129\n",
      "{'in': array([[  3,  77,   8, ...,  17, 224,  38],\n",
      "       [ 12,  16,  55, ...,   4, 317,  14],\n",
      "       [491,   3, 120, ...,  27,  25,  18],\n",
      "       ..., \n",
      "       [ 19,  25,  12, ..., 190, 255, 165],\n",
      "       [ 11,  23,   4, ...,  10,  49, 266],\n",
      "       [ 30,  12,  15, ...,   4,  14,  55]]), 'out': array([[ 77,   8,   3, ..., 224,  38,  12],\n",
      "       [ 16,  55,  46, ..., 317,  14, 491],\n",
      "       [  3, 120,   3, ...,  25,  18,   7],\n",
      "       ..., \n",
      "       [ 25,  12,  10, ..., 255, 165,  11],\n",
      "       [ 23,   4,  19, ...,  49, 266,  30],\n",
      "       [ 12,  15,  13, ...,  14,  55, 109]]), 'len': array([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.text2id('ã“ã‚“ã«ã¡ã¯ğŸ˜'))\n",
    "print(tokenizer.id2text([33, 27, 8, 51, 14, 3]))\n",
    "print('batch list num: {}'.format(len(batch_list)))\n",
    "print(batch_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 50\n",
    "save_path = 'tmp/rnnlm/model.ckpt'\n",
    "log_dir = 'tmp/rnnlm/log/'\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.dirname(save_path)):\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss_op, global_step=global_step)\n",
    "tf.summary.scalar('loss', loss_op)\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 100000.0\n",
    "sess = tf.Session()\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(max_epoch):\n",
    "    random.shuffle(batch_list)\n",
    "    for batch in batch_list:\n",
    "        feed_dict = {\n",
    "            in_ph: batch['in'],\n",
    "            out_ph: batch['out'],\n",
    "            len_ph: batch['len'],\n",
    "        }\n",
    "        _, loss, summary, step = sess.run([train_op, loss_op, summary_op, global_step], feed_dict)\n",
    "        summary_writer.add_summary(summary, step)\n",
    "        if loss < min_loss:\n",
    "            saver.save(sess, save_path)\n",
    "            min_loss = loss\n",
    "    print('epoch {}/{} - loss: {}'.format(epoch, max_epoch, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from learned_model/rnnlm/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "load_path = 'learned_model/rnnlm/model.ckpt'\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char = 'ç§'\n",
    "generated_ids = sess.run(generate_op, {\n",
    "    gen_start_token_ph:  tokenizer.text2id(start_char)[0]\n",
    "})[0, :, 0]\n",
    "generated_text = start_char + tokenizer.id2text(generated_ids)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
